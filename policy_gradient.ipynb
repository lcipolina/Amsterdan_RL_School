{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lcipolina/Amsterdan_RL_School/blob/main/policy_gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b0c7f42",
      "metadata": {
        "id": "1b0c7f42"
      },
      "source": [
        "# Policy Gradient --- Practical Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "11a6f303",
      "metadata": {
        "id": "11a6f303"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as random\n",
        "from jax import grad\n",
        "jax.config.update('jax_platform_name', 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef98bfe8",
      "metadata": {
        "id": "ef98bfe8"
      },
      "source": [
        "## Setting up the RL task with Openai Gym\n",
        "\n",
        "Gym (https://www.gymlibrary.ml/) is the de facto standard interface for simulating **Markov Decision Processes** (MDPs).\n",
        "\n",
        "An MDP is made of \n",
        "* a **state space** $\\mathcal{S}$\n",
        "* an **action space** $\\mathcal{A}$\n",
        "* a **starting-state distribution** $\\mu(s)$\n",
        "* a **transition function** $p(s'\\mid s,a)$\n",
        "* a **reward function** $r(s,a)$\n",
        "\n",
        "![title](https://drive.google.com/uc?export=view&id=1at4LUww9SweDvitBPEeyOU5h8FlSEVdd)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dcf9c0b",
      "metadata": {
        "id": "9dcf9c0b"
      },
      "source": [
        "Let's start by creating an MDP (our **environment** or reinforcement learning **task**).\n",
        "\n",
        "Gym has several ready-to-use tasks (you can also create custom environments by implementing the gym interface)\n",
        "\n",
        "The `make` method accepts a string that specifies the name and version of the task. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "377b11bb",
      "metadata": {
        "id": "377b11bb"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f02712c",
      "metadata": {
        "id": "1f02712c"
      },
      "source": [
        "**Cart-Pole** is a standard control task where the goal is to balance a pole that is attached to a cart, only by controlling the horizontal motion of the cart.\n",
        "\n",
        "![title](https://drive.google.com/uc?export=view&id=10TI1bF3Fcssy61dnzDemH4ZoY7LMS4Tf)\n",
        "\n",
        "Let's have a look at the **state space**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b539db25",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b539db25",
        "outputId": "694c2ad2-8d99-4904-adfa-2064856bce80"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd1befbe",
      "metadata": {
        "id": "bd1befbe"
      },
      "source": [
        "In this MDP, a state is a vector of 4 real numbers (cart position, cart velocity, pole angle, pole angular velocity). We call this a *continuous* state space.\n",
        "\n",
        "Let's take note of the state dimension:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85335abd",
      "metadata": {
        "id": "85335abd"
      },
      "outputs": [],
      "source": [
        "state_dim = env.observation_space.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a7f119b",
      "metadata": {
        "id": "7a7f119b"
      },
      "source": [
        "Now let's have a look at the **action space**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15362234",
      "metadata": {
        "id": "15362234"
      },
      "outputs": [],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "335850e0",
      "metadata": {
        "id": "335850e0"
      },
      "source": [
        "We only have two actions, `0` (push cart to the left) and `1` (push cart to the right). This is a *finite* action space. \n",
        "\n",
        "Let's take note of the number of actions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cad9477",
      "metadata": {
        "id": "0cad9477"
      },
      "outputs": [],
      "source": [
        "n_actions = env.action_space.n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "518c6575",
      "metadata": {
        "id": "518c6575"
      },
      "source": [
        "The **starting-state distribution** is given by the `reset` method. It returns the initial state, which is a random vector not too far from zero, corresponding to the cart being almost in the center, the pole being almost vertical, and both almost still."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cb077ff",
      "metadata": {
        "id": "0cb077ff"
      },
      "outputs": [],
      "source": [
        "env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48bd58c5",
      "metadata": {
        "id": "48bd58c5"
      },
      "source": [
        "Both the **transition function** and the **reward function** are implemented by the `step` method. It takes an action and it returns the next state, the reward, a termination or \"done\" flag and a side-information dictionary (the latter is empty for Cart-Pole)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ee71426",
      "metadata": {
        "id": "6ee71426"
      },
      "outputs": [],
      "source": [
        "env.step(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "818436f3",
      "metadata": {
        "id": "818436f3"
      },
      "source": [
        "Note that the environment object is *stateful*: the next state and reward depend on the (internal) current state and the given action.\n",
        "\n",
        "In Cartpole, reward is 1 at every step (if the pole is up, you are doing good)\n",
        "\n",
        "The episode terminates (`step` returns a `True` done flag) when the pole falls outside a certain angle or the cart goes too far to the left or to the right.\n",
        "\n",
        "See the docs for more detailed info on the Cart-Pole task (https://www.gymlibrary.ml/environments/classic_control/cart_pole/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c17a61c9",
      "metadata": {
        "id": "c17a61c9"
      },
      "source": [
        "## A simple agent interface\n",
        "\n",
        "We have our environment, now let's build an **agent**. \n",
        "\n",
        "Our agent will `act` depending on the current state and `update` its behavior based on experience (learn).\n",
        "\n",
        "Our default implementation is an agent that does random actions and does not learn anything. This agent has a fixed **stochastic policy**\n",
        "\n",
        "$\\LARGE\\mathbb{P}(A_t=a\\mid S_t=s) = \\pi(a\\mid s)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4adeeb7",
      "metadata": {
        "id": "d4adeeb7"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    \"\"\"\n",
        "    A (learning) agent\n",
        "    \n",
        "    Attributes\n",
        "    seed: random seed (for reproducibility)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, seed=0):\n",
        "        self.reset(seed=seed)\n",
        "    \n",
        "    def reset(self, seed=0):\n",
        "        \"\"\"\n",
        "        Reset policy to default and re-seed the random number generator\n",
        "        \n",
        "        Parameters\n",
        "        seed: random seed (for reproducibility)\n",
        "        \"\"\"\n",
        "        self.key = random.PRNGKey(seed) #jax RNG bookkeeping (not important)\n",
        "    \n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        The current policy of the agent.\n",
        "        \n",
        "        Parameters\n",
        "        state: the current state of the MDP\n",
        "        \n",
        "        Returns: the selected action\n",
        "        \"\"\"\n",
        "        self.key, subkey = random.split(self.key) #jax RNG bookkeeping (not important)\n",
        "        action_probs = jnp.ones(n_actions) / n_actions\n",
        "        action = random.choice(subkey, n_actions, p=action_probs).item() # sample (random) action\n",
        "        return action\n",
        "    \n",
        "    def update(self, state, action, discounted_reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Update the agent's policy given one piece of interaction data\n",
        "        \n",
        "        Parameters\n",
        "        state: last state in which an action was selected\n",
        "        action: last action selected by the agent\n",
        "        discounted_reward: last reward received by the agent, already discounted\n",
        "        next_state: state resulting from the agent's last action\n",
        "        done: True if next_state is a terminal state\n",
        "        \n",
        "        Returns: None\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ab9f34a",
      "metadata": {
        "id": "2ab9f34a"
      },
      "source": [
        "## The interaction loop"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef54d0cf",
      "metadata": {
        "id": "ef54d0cf"
      },
      "source": [
        "The following function makes the agent interact with the environment over several episodes and records the **performance** of the agent at each episode\n",
        "\n",
        "$\\LARGE J(\\pi)=\\mathbb{E}_{S_0\\sim\\mu, A_t\\sim\\pi(\\cdot|S_t),S_{t+1}\\sim p(\\cdot|S_t,A_t)}\\left[\\sum_{t=0}^\\infty\\gamma^t R_{t+1}\\right]$\n",
        "\n",
        "We can use *random seeds* to make the experiment fully reproducible. \n",
        "\n",
        "The code for randomization is already here. If you want to know more about random numbers in Jax see https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#jax-prng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eca87f0e",
      "metadata": {
        "id": "eca87f0e"
      },
      "outputs": [],
      "source": [
        "def run_interaction(env, agent, n_steps=10**5, discount=0.99, seed=None, render=False):\n",
        "    \"\"\"\n",
        "    Simulates several episodes of interaction between an MDP and an (learning) agent\n",
        "    \n",
        "    Parameters\n",
        "    env: the MDP\n",
        "    agent: the agent\n",
        "    n_steps: total number of interaction time-steps (may result in multiple episodes)\n",
        "    seed: random seed\n",
        "    render: if True, the interaction is displayed graphically\n",
        "    \n",
        "    Returns: a list with the agent's performance for each episode\n",
        "    \"\"\"    \n",
        "    performance = 0.\n",
        "    performances = [] #we will store the performance of each episode\n",
        "    state = env.reset(seed=seed) #sample initial state\n",
        "    last_reset = 0 #starting timestep of the current episode\n",
        "    episode = 1 #index of current episode\n",
        "    \n",
        "    for t in range(n_steps): #interaction loop\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        discounted_reward = reward * discount**(t - last_reset) #discounting is from the start of the episode\n",
        "        performance += discounted_reward\n",
        "        \n",
        "        #Display interaction\n",
        "        if render:\n",
        "            env.render()\n",
        "            \n",
        "        #Update the agent's policy with the latest piece of interaction data\n",
        "        agent.update(state, action, discounted_reward, next_state, done)\n",
        "        \n",
        "        if done: #go to next episode\n",
        "            print('Episode %d: performance = %.2f' % (episode, performance))\n",
        "            episode += 1\n",
        "            performances.append(performance)\n",
        "            performance = 0\n",
        "            state = env.reset()\n",
        "            last_reset = t + 1\n",
        "        else: #go to next state\n",
        "            state = next_state\n",
        "        \n",
        "    env.close()\n",
        "    return performances"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbf9218f",
      "metadata": {
        "id": "bbf9218f"
      },
      "source": [
        "Let's simulate the interaction of the random agent with the cartpole environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01a1eaf1",
      "metadata": {
        "id": "01a1eaf1"
      },
      "outputs": [],
      "source": [
        "random_agent = Agent(seed=0)\n",
        "performances = run_interaction(env, random_agent, n_steps=1000, render=False, seed=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7cb0c03",
      "metadata": {
        "id": "a7cb0c03"
      },
      "source": [
        "Let's plot the performance over the episodes. This is called a **learning curve**, but this agent cannot learn anything"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0d76124",
      "metadata": {
        "id": "b0d76124"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt #matlab-like plots https://matplotlib.org/stable/tutorials/introductory/pyplot.html\n",
        "\n",
        "plt.plot(performances)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Performance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f166e4d4",
      "metadata": {
        "id": "f166e4d4"
      },
      "source": [
        "## Policy Gradient: theory recap\n",
        "\n",
        "We fix a parametric policy class\n",
        "\n",
        "$\\LARGE \\Pi=\\{\\pi_\\theta\\mid\\theta\\in\\Theta\\}$\n",
        "\n",
        "We use the following shorthand for the performance measure:\n",
        "\n",
        "$\\LARGE J(\\theta) = J(\\pi_\\theta)$\n",
        "\n",
        "And learn policy parameters by **gradient ascent**\n",
        "\n",
        "$\\LARGE \\theta\\gets\\theta+\\alpha\\nabla_\\theta J(\\theta)$\n",
        "\n",
        "where $\\alpha$ is the **learning rate** and the expression of the gradient is given by the **policy gradient theorem**\n",
        "\n",
        "$\\Large\\nabla_\\theta J(\\theta) = (1-\\gamma)^{-1}\\mathbb{E}_{S\\sim d_\\theta, A\\sim\\pi_\\theta(\\cdot| S)}\\left[\\nabla_\\theta\\log\\pi_\\theta(A| S)Q^\\theta(S,A)\\right]$\n",
        "\n",
        "where the state-action value function is:\n",
        "\n",
        "$\\LARGE Q^\\theta(s,a) = \\mathbb{E}_\\theta\\left[\\sum_{t=0}^\\infty\\gamma^t R_{t+1}\\bigg|S_0=s, A_0=a\\right]$\n",
        "\n",
        "and the discounted state-occupancy measure is:\n",
        "\n",
        "$\\LARGE d_\\theta(s)=(1-\\gamma)\\sum_{t=0}^\\infty\\gamma^t\\mathbb{P}_\\theta(S_t=s)$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e62dacdc",
      "metadata": {
        "id": "e62dacdc"
      },
      "source": [
        "## JAX quickstart\n",
        "\n",
        "JAX is a numerical computing and automatic differentiation library.\n",
        "\n",
        "The basic objects are (multi-dimensional) arrays (same as in *numpy*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "562ca0ac",
      "metadata": {
        "id": "562ca0ac"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp #same as numpy!\n",
        "\n",
        "v = jnp.array([0.3, 0.2, 0.5]) #a 1-D array (a vector)\n",
        "A = jnp.array([[1.,-1.,0],[0,1.,1.]]) # a 2-D array (a matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16886cd2",
      "metadata": {
        "id": "16886cd2"
      },
      "source": [
        "We can access arrays for reading values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97185205",
      "metadata": {
        "id": "97185205"
      },
      "outputs": [],
      "source": [
        "v[1] #second element of the vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5e47e72",
      "metadata": {
        "id": "b5e47e72"
      },
      "outputs": [],
      "source": [
        "A[0,1] #matrix element in first row and second column"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a13011f",
      "metadata": {
        "id": "7a13011f"
      },
      "source": [
        "For advanced array reading, see slicing in numpy https://www.w3schools.com/python/numpy/numpy_array_slicing.asp.\n",
        "\n",
        "For example we can reverse an array as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c11aca55",
      "metadata": {
        "id": "c11aca55"
      },
      "outputs": [],
      "source": [
        "v[::-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ea4b31e",
      "metadata": {
        "id": "6ea4b31e"
      },
      "source": [
        "**Important**: jax arrays are *immutable*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cb71b71",
      "metadata": {
        "id": "3cb71b71"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    v[1] = 0. #attempt to assign new value to array element\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d753e4e5",
      "metadata": {
        "id": "d753e4e5"
      },
      "source": [
        "You must create a new array instead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96878e58",
      "metadata": {
        "id": "96878e58"
      },
      "outputs": [],
      "source": [
        "w = v.at[1].set(0.)\n",
        "w"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58cb1a91",
      "metadata": {
        "id": "58cb1a91"
      },
      "source": [
        "We can perform linear algebra on arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9dfe96c",
      "metadata": {
        "id": "c9dfe96c"
      },
      "outputs": [],
      "source": [
        "#Elementwise operations (the result is always a new array)\n",
        "print(v+v)\n",
        "print(2*v)\n",
        "print(v*v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5e0da07",
      "metadata": {
        "id": "f5e0da07"
      },
      "outputs": [],
      "source": [
        "A @ v #matrix multiplication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9600048d",
      "metadata": {
        "id": "9600048d"
      },
      "outputs": [],
      "source": [
        "jnp.dot(v,v) #dot product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84d6f6ef",
      "metadata": {
        "id": "84d6f6ef"
      },
      "outputs": [],
      "source": [
        "A.T #matrix transpose"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d690c9e",
      "metadata": {
        "id": "4d690c9e"
      },
      "source": [
        "We can easily **differentiate** functions with `grad`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97c6dd7c",
      "metadata": {
        "id": "97c6dd7c"
      },
      "outputs": [],
      "source": [
        "from jax import grad\n",
        "\n",
        "def f(x):\n",
        "    return jnp.dot(x, x)\n",
        "\n",
        "nabla_f = grad(f) #gradient of f w.r.t. x (itself a function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa580ad2",
      "metadata": {
        "id": "aa580ad2"
      },
      "outputs": [],
      "source": [
        "nabla_f(v) #the gradient of f evaluated at x=v (the gradient of x^Tx is 2x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4829dc1",
      "metadata": {
        "id": "a4829dc1"
      },
      "source": [
        "Finally, some useful jax functions (you may need them for the exercises)\n",
        "\n",
        "* `jnp.dot` : dot product https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.dot.html\n",
        "* `jnp.log` : natural logarithm https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.log.html\n",
        "* `jnp.exp` : exponential https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.exp.html\n",
        "* `jnp.sum` : sum of array elements https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.sum.html\n",
        "* `jnp.cumsum` : cumulative sum https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.cumsum.html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d04689ef",
      "metadata": {
        "id": "d04689ef"
      },
      "source": [
        "## EXERCISE 1: Softmax policy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "708dfa77",
      "metadata": {
        "id": "708dfa77"
      },
      "source": [
        "It's your turn to implement an agent that acts according to a parametric policy\n",
        "\n",
        "The **Softmax policy** is often used for finite actions. A possible formulation is:\n",
        "\n",
        "$\\Huge \\pi_\\theta(a\\mid s) = \\frac{\\exp(\\theta_a\\cdot s)}{\\sum_{a'\\in\\mathcal{A}}\\exp(\\theta_{a'}\\cdot s)}$\n",
        "\n",
        "where the policy parameter $\\theta$ is a 2-D array (matrix) of size n_actions $\\times$ state_dim and $\\theta_a$ denotes the row of $\\theta$ corresponding to action $a$. \n",
        "\n",
        "This is called a *linear* softmax policy because, for each action, the (unnormalized) log-probability is given by a linear combination of $\\theta_a$ and the current state.\n",
        "\n",
        "To implement the softmax policy, you just need to implement the `policy` method in the code below, which returns the probability of playing each action given the policy parameters and the state.\n",
        "\n",
        "*Suggestion: implement the log-action probabilities first in the* `log_policy` *method*\n",
        "\n",
        "Note that, even if the SoftmaxAgent object stores the current policy parameters as `self.policy_parameters`, the `policy` and `log_policy` methods take parameters as an input. This is to allow differentiation with JAX in the following steps. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53fa062a",
      "metadata": {
        "id": "53fa062a"
      },
      "outputs": [],
      "source": [
        "class SoftmaxAgent(Agent):\n",
        "    \"\"\"\n",
        "    Agent implementing a fixed linear-Softmax policy.\n",
        "    \n",
        "    Attributes\n",
        "    state_dim: dimension of state vector\n",
        "    n_actions: number of available actions\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim, n_actions, seed=0):\n",
        "        self.state_dim = state_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.reset(seed=seed)\n",
        "        \n",
        "    def reset(self, seed=0):\n",
        "        \"\"\"Set policy parameters to all zeros and re-seed random number generator\"\"\"\n",
        "        self.key = random.PRNGKey(seed)\n",
        "        self.policy_parameters = jnp.zeros((self.n_actions, self.state_dim))\n",
        "    \n",
        "    def log_policy(self, policy_parameters, state):\n",
        "        \"\"\"\n",
        "        Logarithm of action probability given policy parameters and state\n",
        "        \n",
        "        Parameters\n",
        "        policy_parameters: parameters (theta) of the policy to evaluate\n",
        "        state: the current state\n",
        "        \n",
        "        Returns\n",
        "        a vector of log-probabilities, one for each action\n",
        "        \"\"\"\n",
        "        state_features = jnp.array(state) #let's make sure the state is a jax array\n",
        "        \n",
        "        #YOUR SOLUTION HERE\n",
        "    \n",
        "    def policy(self, policy_parameters, state):\n",
        "        \"\"\"\n",
        "        Action probability given policy parameters and state\n",
        "        \n",
        "        Parameters\n",
        "        policy_parameters: parameters (theta) of the policy to evaluate\n",
        "        state: the current state\n",
        "        \n",
        "        Returns\n",
        "        a vector of probabilities, one for each action\n",
        "        \"\"\"\n",
        "        #YOUR SOLUTION HERE\n",
        "    \n",
        "    def act(self, state):\n",
        "        self.key, subkey = random.split(self.key)\n",
        "        action_probs = self.policy(self.policy_parameters, state)\n",
        "        action = random.choice(subkey, n_actions, p=action_probs).item()\n",
        "        return action\n",
        "    \n",
        "    def score_function(self, policy_parameters, state, action):\n",
        "        \"\"\"\n",
        "        Gradient (w.r.t. policy parameters) of log-probability of a fixed action, given policy parameters and state\n",
        "        \n",
        "        Parameters:\n",
        "        policy_parameters: parameters (theta) of the policy to evaluate\n",
        "        state: the current state\n",
        "        action: the selected action\n",
        "        \n",
        "        Returns: the gradient (it has the same shape as self.policy_parameters) \n",
        "        \"\"\"\n",
        "        #partial solution, only the shape is correct. REPLACE WITH YOUR SOLUTION\n",
        "        return jnp.zeros(self.policy_parameters.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dab38281",
      "metadata": {
        "id": "dab38281"
      },
      "source": [
        "The Softmax policy with all parameters set to zero is again the random policy! To test your implementation, you can inspect the action probabilities it produces and/or run it in the Cart-Pole task using `run interaction`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a3d1b96",
      "metadata": {
        "id": "1a3d1b96"
      },
      "source": [
        "## EXERCISE 2: Score function\n",
        "\n",
        "To compute the policy gradient, we just need to differentiate the log-action-probability. This gradient is called **score function**\n",
        "\n",
        "$\\Huge \\nabla_\\theta\\log\\pi_\\theta(a|s)$\n",
        "\n",
        "A stub of the `score_function` method is given.\n",
        "\n",
        "**Hint 1:** if you haven't implemented the `log_policy` method yet, it is a good time to do it!\n",
        "\n",
        "**Hint 2:** `log_policy` has an another argument than the policy parameters (the state). By default, `grad` differentiates w.r.t. the *first* function argument\n",
        "\n",
        "**Hint 3:** `log_policy` returns a vector of probabilities, one for each action, but `grad` can only differentiate scalar-valued functions. You may want to index the probability vector *before* differentiating. JAX encourages a functional-programming style: use `lambda` or nested functions to define exactly what you want to differentiate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7701a6ef",
      "metadata": {
        "id": "7701a6ef"
      },
      "source": [
        "## EXERCISE 3: REINFORCE\n",
        "\n",
        "Finally, we need to estimate the policy gradient from data. The simplest way is given by the REINFORCE estimator (here given in the \"PGT\" variant):\n",
        "\n",
        "$\\LARGE \\widehat{\\nabla}_\\theta J(\\theta)=\\sum_{t=0}^{T-1}\\nabla_\\theta\\log\\pi_\\theta(A_t\\mid S_t)\\sum_{h=t}^{T-1}\\gamma^h R_{h+1}$\n",
        "\n",
        "Implement the gradient ascent update in the `update` method using the REINFORCE gradient estimator and the `score_function` method you have implemented before.\n",
        "\n",
        "You may need to play with the `learning_rate` parameter. A larger learning rate can lead to faster convergence but also to instabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1086e76",
      "metadata": {
        "id": "f1086e76"
      },
      "outputs": [],
      "source": [
        "class PGSoftmaxAgent(SoftmaxAgent):\n",
        "    \"\"\"\n",
        "    Agent learning a linear-Softmax policy with REINFORCE\n",
        "    \n",
        "    Attributes\n",
        "    state_dim: dimension of state vector\n",
        "    n_actions: number of available actions\n",
        "    discount: discount factor of rewards\n",
        "    learning_rate: learning rate of the gradient-ascent update (alpha)\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim, n_actions, seed=0, discount=0.99, learning_rate=1e-3):\n",
        "        super().__init__(state_dim, n_actions, seed)\n",
        "        self.discount = discount\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "    def reset(self, seed=0):\n",
        "        super().reset(seed=seed)\n",
        "        #bookkeeping:\n",
        "        self.discounted_rewards = [] #will store discounted rewards for each timestep, just for current episode\n",
        "        self.scores = [] #will store scores (grad-log-pi) for each timestep, just for current episode\n",
        "    \n",
        "    def update(self, state, action, discounted_reward, next_state, done):\n",
        "        #bookkeeping:\n",
        "        current_param = self.policy_parameters\n",
        "        self.discounted_rewards.append(discounted_reward)\n",
        "        score = self.score_function(current_param, state, action) #calls the method you have implemented in Ex. 2\n",
        "        self.scores.append(score)\n",
        "        \n",
        "        if done:\n",
        "            #At the end of each episode, update policy parameters with estimated policy gradient            \n",
        "            #YOUR SOLUTION HERE\n",
        "            \n",
        "            #bookkeping: data are discarded after each policy update\n",
        "            self.discounted_rewards = []\n",
        "            self.scores = []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f3710b5",
      "metadata": {
        "id": "2f3710b5"
      },
      "source": [
        "Time to test you learning agent! You should see a small improvement w.r.t. the random policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81028493",
      "metadata": {
        "id": "81028493"
      },
      "outputs": [],
      "source": [
        "reinforce_agent = PGSoftmaxAgent(state_dim, n_actions, seed=0, learning_rate=1e-3)\n",
        "performances = run_interaction(env, reinforce_agent, n_steps=10000, render=False)\n",
        "plt.plot(performances)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ce9d35b",
      "metadata": {
        "id": "1ce9d35b"
      },
      "source": [
        "You should notice a slight increase in the performance...but what behavior has the agent actually learned? It is a good practice to render the behavior of the learned policy. You should compare it with the random policy that was defined at the beginning of the tutorial.\n",
        "\n",
        "*Random policy:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65e5f7c4",
      "metadata": {
        "id": "65e5f7c4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "env = gym.make(\"CartPole-v1\") #create a new environment to avoid pygame display errors\n",
        "run_interaction(env, random_agent, n_steps=200, render=True, seed=0); #display behavior of random policy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9dd9dea",
      "metadata": {
        "id": "e9dd9dea"
      },
      "source": [
        "*Learned policy:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b12583e",
      "metadata": {
        "id": "8b12583e"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "env = gym.make(\"CartPole-v1\") #create a new environment to avoid pygame display errors\n",
        "run_interaction(env, reinforce_agent, n_steps=500, render=True, seed=0); #display behavior of learned policy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b76ecd9",
      "metadata": {
        "id": "2b76ecd9"
      },
      "source": [
        "## Extras\n",
        "\n",
        "There are several ways you can improve your learning agent!\n",
        "\n",
        "### Computational efficiency\n",
        "\n",
        "* Jax offers many ways to speed up computations https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#using-jit-to-speed-up-functions\n",
        "* The action sampling of the Softmax policy can be made more efficient using the Gumbel trick https://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/. The Gumbel distribution is already available in `jax.random` https://jax.readthedocs.io/en/latest/_autosummary/jax.random.gumbel.html\n",
        "\n",
        "### Variance reduction\n",
        "* Variance of the estimated gradients is one of the main drawbacks of policy gradient algorithms. A first way to reduce variance is to increase the **batch size**. You just need to collect $N>1$ trajectories for each policy update (e.g. $N=100$) and then average the $N$ gradient estimates. Here superscripts are used to index trajectories.\n",
        "<br />\n",
        "$\\Large \\widehat{\\nabla}_\\theta J(\\theta)=\\frac{1}{N}\\sum_{i=1}^N\\sum_{t=0}^{T-1}\\nabla_\\theta\\log\\pi_\\theta(A_t^{(i)}\\mid S_t^{(i)})\\sum_{h=t}^{T-1}\\gamma^h R_{h+1}^{(i)}$\n",
        "\n",
        "* You can then add a **baseline** to your gradient estimator:<newline/>\n",
        "    $\\Large \\widehat{\\nabla}_\\theta J(\\theta)=\\frac{1}{N}\\sum_{i=1}^N\\sum_{t=0}^{T-1}\\nabla_\\theta\\log\\pi_\\theta(A_t^{(i)}\\mid S_t^{(i)})\\sum_{h=t}^{T-1}\\left(\\gamma^h R_{h+1}^{(i)} - b_h\\right)$ \n",
        "<br />\n",
        "Any baseline that does not depend on actions keeps the gradient estimator unbiased, and a good baseline can reduce variance. A common choice is the average-reward baseline \n",
        "<br />\n",
        "$\\Large b_h = \\frac{1}{N}\\sum_{i=1}^N\\sum_{h=t}^{T-1}\\gamma^h R_{h+1}^{(i)}$\n",
        "<br />\n",
        "See <a href=\"https://www.sciencedirect.com/science/article/pii/S0893608008000701\">this paper</a> for more sophisticated variance-reduction baselines\n",
        "\n",
        "\n",
        "* **Actor critic (advanced)**: instead of estimating the Q functions with the return-to-go, you can learn it using your favourite policy evaluation algorithm\n",
        "<br />\n",
        "$\\Large \\widehat{\\nabla}_\\theta J(\\theta)=\\sum_{t=0}^{T-1}\\nabla_\\theta\\log\\pi_\\theta(A_t\\mid S_t)\\hat{Q}_\\theta(S_t, A_t)$\n",
        "<br />\n",
        "Designing actor-critic algorithms is a subtle art. A general principle is that the critic (the approximated Q function) should be updated at a faster rate than the actor (the policy), e.g. by using a larger learning rate.\n",
        "<a href=\"https://papers.nips.cc/paper/1999/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html\">Here</a> is a good reference \n",
        "\n",
        "\n",
        "### Policy class\n",
        "* **State features**: The policy class we have considered so far may be too simple to achieve optimal performance. You may try to handcraft some state features $\\phi:\\mathcal{S}\\to\\mathbb{R}^d$ and use the following policy class\n",
        "<br />\n",
        "$\\Large \\pi_\\theta(a\\mid s) = \\frac{\\exp(\\theta_a\\cdot \\phi(s))}{\\sum_{a'\\in\\mathcal{A}}\\exp(\\theta_{a'}\\cdot \\phi(s))}$\n",
        "\n",
        "* **Go deep! (advanced)** If you can't think of good features, you can try to implement a deep policy class where the exponential weights are given by a neural network that takes states as inputs, and the policy parameters are the weights of the network. Here is a possible partial implementation using `stax` (https://jax.readthedocs.io/en/latest/jax.example_libraries.stax.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f55505cd",
      "metadata": {
        "id": "f55505cd"
      },
      "outputs": [],
      "source": [
        "from jax.example_libraries import stax\n",
        "\n",
        "class DeepAgent(PGSoftmaxAgent):\n",
        "    def __init__(self, state_dim, n_actions, hidden=8):\n",
        "        #neural network with one hidden layer\n",
        "        self.nn_init, self.model = stax.serial(\n",
        "            stax.Dense(hidden), #state as input\n",
        "            stax.Tanh, #tanh activations\n",
        "            stax.Dense(n_actions)) #one output per action (the unnormalized log-probabilities)\n",
        "        super().__init__(state_dim, n_actions)\n",
        "        \n",
        "    def reset(self, seed=0):\n",
        "        key = random.PRNGKey(seed)\n",
        "        output_shape, params_init = self.nn_init(key, (-1, self.state_dim)) #initialization of network weights\n",
        "        self.param = params_init #policy parameters are network weights\n",
        "    \n",
        "    def log_policy(self, policy_parameters, state):\n",
        "        x = jnp.array(state)\n",
        "        unn_log_probs = self.model(policy_parameters, x) #the network gives the unnormalized log probabilities\n",
        "        #..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95ba73c5",
      "metadata": {
        "id": "95ba73c5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "name": "policy_gradient.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "c17a61c9",
        "2ab9f34a",
        "f166e4d4",
        "e62dacdc",
        "d04689ef",
        "1a3d1b96",
        "7701a6ef",
        "2b76ecd9"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}